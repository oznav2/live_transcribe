services:
  transcription-app-ivrit:
    build:
      context: .
      dockerfile: Dockerfile.ivrit
      target: base
    container_name: live-transcription-ivrit
    ports:
      - "8009:8009"
    # Load environment variables from .env
    env_file:
      - .env
    environment:
      # Model selection - using faster_whisper CT2 models for best quality
      # Options: 
      #   - ivrit-ct2 (default, best for Hebrew transcription)
      #   - ivrit-v3-turbo (alias for ivrit-ct2)
      #   - whisper-v3-turbo (general v3 turbo model)
      - WHISPER_MODEL=ivrit-ct2
      - IVRIT_MODEL_NAME=ivrit-ai/whisper-large-v3-turbo-ct2
      - IVRIT_MODEL_TYPE=ct2
      - IVRIT_DEVICE=cuda
      - IVRIT_COMPUTE_TYPE=float16
      - IVRIT_BEAM_SIZE=5
      # Port configuration
      - PORT=8009
      # CUDA settings
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      # Ensure proper library paths
      - LD_LIBRARY_PATH=/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cublas/lib
      # Deepgram config (from .env if available)
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - DEEPGRAM_TIME_LIMIT=${DEEPGRAM_TIME_LIMIT:-3600}
      - DEEPGRAM_MODEL=${DEEPGRAM_MODEL:-nova-3}
      - DEEPGRAM_LANGUAGE=${DEEPGRAM_LANGUAGE:-en-US}
      # Audio caching
      - AUDIO_CACHE_ENABLED=true
      # Python settings
      - PYTHONUNBUFFERED=1
      # Optional: Enable diarization (speaker detection)
      - IVRIT_ENABLE_DIARIZATION=${IVRIT_ENABLE_DIARIZATION:-false}
    volumes:
      # Model cache - persists downloaded models
      - whisper-models-ivrit:/root/.cache/whisper
      - huggingface-cache:/root/.cache/huggingface
      # Application cache directories
      - ./cache:/app/cache
      # Logs
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; r = requests.get('http://localhost:8009/health', timeout=10); r.raise_for_status()"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 16G  # Ivrit models need more memory
    runtime: nvidia
    shm_size: '2gb'  # Shared memory for PyTorch

  # Optional: Separate service for API-only mode (no GPU needed)
  transcription-api:
    build:
      context: .
      dockerfile: Dockerfile.ivrit
      target: base
    container_name: live-transcription-api
    ports:
      - "8010:8009"
    env_file:
      - .env
    environment:
      # Use Deepgram for API-only mode
      - WHISPER_MODEL=deepgram
      - PORT=8009
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - DEEPGRAM_MODEL=${DEEPGRAM_MODEL:-nova-3}
      - DEEPGRAM_LANGUAGE=${DEEPGRAM_LANGUAGE:-en-US}
      - AUDIO_CACHE_ENABLED=true
      - PYTHONUNBUFFERED=1
    volumes:
      - ./cache:/app/cache
      - ./logs:/app/logs
    restart: unless-stopped
    profiles:
      - api-only

volumes:
  whisper-models-ivrit:
    driver: local
  huggingface-cache:
    driver: local