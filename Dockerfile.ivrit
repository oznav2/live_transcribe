# Enhanced Dockerfile with Ivrit AI models support
# Based on PyTorch with CUDA 12.1 for optimal performance

# Use PyTorch base image with CUDA 12.1 support (matching ivrit-ai requirements)
FROM pytorch/pytorch:2.4.1-cuda12.1-cudnn9-runtime AS base

# Set working directory
WORKDIR /app

# Configure LD_LIBRARY_PATH for CUDA libraries (critical for ivrit models)
ENV LD_LIBRARY_PATH="/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cublas/lib:${LD_LIBRARY_PATH}"

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    curl \
    wget \
    build-essential \
    cmake \
    portaudio19-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.ivrit.txt .

# Install Python dependencies
# Install all requirements from requirements.ivrit.txt
RUN pip install --no-cache-dir -r requirements.ivrit.txt

# Install yt-dlp separately to ensure latest version
RUN pip install --no-cache-dir yt-dlp

# NOTE: We do NOT install openai-whisper to avoid conflicts with faster-whisper
# faster-whisper is superior for our use case:
# - 2-4x faster performance
# - Lower memory usage
# - Better optimized for production
# - Ivrit models are in CT2 format specifically for faster-whisper
# The app.py handles missing openai-whisper gracefully and will use faster-whisper

# Pre-download and cache Ivrit models during build
# This ensures models are available immediately at runtime
RUN python3 -c "import faster_whisper; print('Downloading ivrit-ai/whisper-large-v3-turbo-ct2...'); m = faster_whisper.WhisperModel('ivrit-ai/whisper-large-v3-turbo-ct2', device='cpu', compute_type='int8'); print('✓ ivrit-ai/whisper-large-v3-turbo-ct2 model downloaded successfully')"

# Also download the general large-v3-turbo as backup
RUN python3 -c "import faster_whisper; print('Downloading large-v3-turbo...'); m = faster_whisper.WhisperModel('large-v3-turbo', device='cpu', compute_type='int8'); print('✓ large-v3-turbo model downloaded successfully')"

# Optional: Pre-download diarization models (with error handling to prevent build failure)
RUN python3 -c "import pyannote.audio; print('Loading speaker diarization model...'); p = pyannote.audio.Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=False)" || echo "Diarization model download skipped - will download at runtime"

# Verify installations
RUN python3 -c "import torch; print('PyTorch version:', torch.__version__, 'CUDA available:', torch.cuda.is_available())"
RUN python3 -c "import faster_whisper; print('faster_whisper installed successfully')"
RUN python3 -c "import ivrit; print('ivrit package installed successfully')"
RUN python3 -c "from deepgram import DeepgramClient; print('Deepgram SDK installed')"
RUN python3 -c "try: import whisper; print('WARNING: openai-whisper found - this may conflict with faster-whisper'); except: print('✓ openai-whisper not installed - using faster-whisper only (recommended)')"

# Copy application code
COPY app.py .
COPY static/ ./static/

# Create necessary directories
RUN mkdir -p /root/.cache/whisper && \
    mkdir -p /app/models && \
    mkdir -p /app/cache/audio && \
    mkdir -p /app/cache/downloads && \
    mkdir -p /app/cache/captures && \
    mkdir -p /app/logs

# OPTIONAL: whisper.cpp support (not required for Ivrit CT2 models)
# The app will work without whisper.cpp - it's only needed for GGML models
# Note: whisper.cpp/GGML support is being phased out in favor of faster_whisper/CT2 models
# which provide better performance and quality

# Create directory structure (will remain empty if whisper.cpp not available)
RUN mkdir -p /app/whisper.cpp/build/bin /app/whisper.cpp/build/src /app/whisper.cpp/build/ggml/src

# Note: To enable GGML model support, you would need to:
# 1. Build whisper.cpp and copy the binaries during docker build
# 2. Copy the GGML model file (e.g., ivrit-whisper-large-v3-turbo.bin)
# Since whisper.cpp is not available by default, GGML models won't be available

# Expose port
EXPOSE 8009

# Environment variables
ENV PYTHONUNBUFFERED=1
ENV PORT=8009
# Default to new Ivrit CT2 model (faster_whisper based - no whisper.cpp needed)
ENV WHISPER_MODEL=ivrit-ct2
ENV IVRIT_MODEL_NAME=ivrit-ai/whisper-large-v3-turbo-ct2
ENV IVRIT_MODEL_TYPE=ct2
ENV IVRIT_DEVICE=cuda
ENV IVRIT_COMPUTE_TYPE=float16
ENV IVRIT_BEAM_SIZE=5
# Audio processing
ENV AUDIO_CACHE_ENABLED=true
# CUDA settings
ENV CUDA_VISIBLE_DEVICES=0
ENV NVIDIA_VISIBLE_DEVICES=all
# Note: GGML/whisper.cpp paths are not set by default since whisper.cpp is optional
# To use GGML models, you would need to provide whisper.cpp binaries and set:
# ENV WHISPER_CPP_PATH=/app/whisper.cpp/build/bin/whisper-cli
# ENV IVRIT_MODEL_PATH=/app/models/ivrit-whisper-large-v3-turbo.bin

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=120s --retries=3 \
    CMD python3 -c "import requests; r = requests.get('http://localhost:8009/health', timeout=10); r.raise_for_status()" || exit 1

# Run the application
CMD ["python3", "app.py"]

##############################################
# Multi-stage variant with smaller final image
##############################################
FROM base AS runtime

# Clean up unnecessary files to reduce image size
RUN pip cache purge && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Set the entrypoint
ENTRYPOINT ["python3"]
CMD ["app.py"]