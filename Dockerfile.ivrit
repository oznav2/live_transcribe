# Enhanced Dockerfile with Ivrit AI models support
# Based on PyTorch with CUDA 12.1 for optimal performance

# Use PyTorch base image with CUDA 12.1 support (matching ivrit-ai requirements)
FROM pytorch/pytorch:2.4.1-cuda12.1-cudnn9-runtime AS base

# Set working directory
WORKDIR /app

# Configure LD_LIBRARY_PATH for CUDA libraries (critical for ivrit models)
ENV LD_LIBRARY_PATH="/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cublas/lib:${LD_LIBRARY_PATH}"

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    curl \
    wget \
    build-essential \
    cmake \
    portaudio19-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.ivrit.txt .

# Install Python dependencies
# Install all requirements from requirements.ivrit.txt
RUN pip install --no-cache-dir -r requirements.ivrit.txt

# Install yt-dlp separately to ensure latest version
RUN pip install --no-cache-dir yt-dlp

# Optional: Install openai-whisper for fallback support (with error handling)
# This is installed separately to avoid conflicts with faster-whisper
# The app.py handles missing openai-whisper gracefully
RUN pip install --no-cache-dir openai-whisper==20231117 || echo "openai-whisper installation skipped - app will use faster-whisper models only"

# Pre-download and cache Ivrit models during build (with error handling)
# This ensures models are available immediately at runtime
RUN python3 -c "import faster_whisper; print('Loading ivrit-ai/whisper-large-v3-turbo-ct2...'); m = faster_whisper.WhisperModel('ivrit-ai/whisper-large-v3-turbo-ct2', device='cpu', compute_type='int8')" || echo "Ivrit model download skipped"
RUN python3 -c "import faster_whisper; print('Loading large-v3-turbo...'); m = faster_whisper.WhisperModel('large-v3-turbo', device='cpu', compute_type='int8')" || echo "Large-v3-turbo model download skipped"

# Optional: Pre-download diarization models (with error handling to prevent build failure)
RUN python3 -c "import pyannote.audio; print('Loading speaker diarization model...'); p = pyannote.audio.Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=False)" || echo "Diarization model download skipped - will download at runtime"

# Verify installations
RUN python3 -c "import torch; print('PyTorch version:', torch.__version__, 'CUDA available:', torch.cuda.is_available())"
RUN python3 -c "import faster_whisper; print('faster_whisper installed successfully')"
RUN python3 -c "import ivrit; print('ivrit package installed successfully')"
RUN python3 -c "from deepgram import DeepgramClient; print('Deepgram SDK installed')"
RUN python3 -c "try: import whisper; print('openai-whisper installed successfully'); except: print('openai-whisper not installed - using faster-whisper only')"

# Copy application code
COPY app.py .
COPY static/ ./static/

# Create necessary directories
RUN mkdir -p /root/.cache/whisper && \
    mkdir -p /app/models && \
    mkdir -p /app/cache/audio && \
    mkdir -p /app/cache/downloads && \
    mkdir -p /app/cache/captures && \
    mkdir -p /app/logs

# Copy the old GGML model as fallback (optional)
# Note: GGML model copying is commented out - uncomment and place model file if needed
# COPY models/ivrit-whisper-large-v3-turbo.bin /app/models/

# Copy whisper.cpp binaries if they exist (for fallback to GGML)
# Using RUN commands with conditional copying since COPY doesn't support shell redirection
RUN mkdir -p /app/whisper.cpp/build/bin /app/whisper.cpp/build/src /app/whisper.cpp/build/ggml/src
# Copy whisper.cpp directory if it exists, otherwise create empty structure
RUN if [ -d "whisper.cpp" ]; then \
        cp -r whisper.cpp /tmp/; \
        echo "whisper.cpp directory copied to temp"; \
    else \
        mkdir -p /tmp/whisper.cpp/build/{bin,src,ggml/src}; \
        echo "whisper.cpp not found, created empty structure"; \
    fi
RUN if [ -f /tmp/whisper.cpp/build/bin/whisper-cli ]; then \
        cp /tmp/whisper.cpp/build/bin/whisper-cli /app/whisper.cpp/build/bin/ && \
        echo "whisper-cli copied successfully"; \
    else \
        echo "whisper.cpp not found, skipping"; \
    fi
RUN if [ -f /tmp/whisper.cpp/build/src/libwhisper.so ]; then \
        cp /tmp/whisper.cpp/build/src/libwhisper.so* /app/whisper.cpp/build/src/ 2>/dev/null || true && \
        echo "whisper libs copied successfully"; \
    else \
        echo "whisper.cpp libs not found, skipping"; \
    fi
RUN if [ -f /tmp/whisper.cpp/build/ggml/src/libggml.so ]; then \
        cp /tmp/whisper.cpp/build/ggml/src/libggml*.so* /app/whisper.cpp/build/ggml/src/ 2>/dev/null || true && \
        echo "ggml libs copied successfully"; \
    else \
        echo "ggml libs not found, skipping"; \
    fi
RUN rm -rf /tmp/whisper.cpp

# Expose port
EXPOSE 8009

# Environment variables
ENV PYTHONUNBUFFERED=1
ENV PORT=8009
# Default to new Ivrit CT2 model
ENV WHISPER_MODEL=ivrit-ct2
ENV IVRIT_MODEL_NAME=ivrit-ai/whisper-large-v3-turbo-ct2
ENV IVRIT_MODEL_TYPE=ct2
ENV IVRIT_DEVICE=cuda
ENV IVRIT_COMPUTE_TYPE=float16
ENV IVRIT_BEAM_SIZE=5
# Fallback GGML paths (if needed)
ENV IVRIT_MODEL_PATH=/app/models/ivrit-whisper-large-v3-turbo.bin
ENV WHISPER_CPP_PATH=/app/whisper.cpp/build/bin/whisper-cli
# Audio processing
ENV AUDIO_CACHE_ENABLED=true
# CUDA settings
ENV CUDA_VISIBLE_DEVICES=0
ENV NVIDIA_VISIBLE_DEVICES=all

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=120s --retries=3 \
    CMD python3 -c "import requests; r = requests.get('http://localhost:8009/health', timeout=10); r.raise_for_status()" || exit 1

# Run the application
CMD ["python3", "app.py"]

##############################################
# Multi-stage variant with smaller final image
##############################################
FROM base AS runtime

# Clean up unnecessary files to reduce image size
RUN pip cache purge && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Set the entrypoint
ENTRYPOINT ["python3"]
CMD ["app.py"]