# Enhanced Dockerfile with Ivrit AI models support
# Based on PyTorch with CUDA 12.1 for optimal performance

# Use PyTorch base image with CUDA 12.1 support (matching ivrit-ai requirements)
FROM pytorch/pytorch:2.4.1-cuda12.1-cudnn9-runtime AS base

# Set working directory
WORKDIR /app

# Configure LD_LIBRARY_PATH for CUDA libraries (critical for ivrit models)
ENV LD_LIBRARY_PATH="/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cublas/lib:${LD_LIBRARY_PATH}"

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    curl \
    wget \
    build-essential \
    cmake \
    portaudio19-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.ivrit.txt .

# Install Python dependencies
# Install all requirements from requirements.ivrit.txt
RUN pip install --no-cache-dir -r requirements.ivrit.txt

# Install yt-dlp separately to ensure latest version (critical for YouTube support)
RUN pip install --no-cache-dir --upgrade yt-dlp

# NOTE: We do NOT install openai-whisper to avoid conflicts with faster-whisper
# faster-whisper is superior for our use case:
# - 2-4x faster performance
# - Lower memory usage
# - Better optimized for production
# - Ivrit models are in CT2 format specifically for faster-whisper
# The app.py handles missing openai-whisper gracefully and will use faster-whisper

# Pre-download and cache Ivrit models during build
# This ensures models are available immediately at runtime
RUN python3 -c "import faster_whisper; print('Downloading ivrit-ai/whisper-large-v3-turbo-ct2...'); faster_whisper.WhisperModel('ivrit-ai/whisper-large-v3-turbo-ct2', device='cpu', compute_type='int8'); print('✓ Model downloaded successfully')"

# Also download the general large-v3-turbo as backup
RUN python3 -c "import faster_whisper; print('Downloading large-v3-turbo...'); faster_whisper.WhisperModel('large-v3-turbo', device='cpu', compute_type='int8'); print('✓ Model downloaded successfully')"

# Pre-download diarization models (with error handling to prevent build failure)
# Try to download the models, but don't fail the build if it doesn't work
RUN python3 -c "\
try: \
    from pyannote.audio import Pipeline; \
    import os; \
    token = os.getenv('HUGGINGFACE_TOKEN', 'hf_dummy'); \
    print('Attempting to download diarization models...'); \
    try: \
        pipeline = Pipeline.from_pretrained('ivrit-ai/pyannote-speaker-diarization-3.1', use_auth_token=token); \
        print('✓ Ivrit diarization model downloaded'); \
    except: \
        try: \
            pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=token); \
            print('✓ Standard diarization model downloaded'); \
        except: \
            print('⚠ Diarization models will be downloaded at runtime'); \
except Exception as e: \
    print(f'⚠ Pyannote not available or models cannot be pre-downloaded: {e}')" || echo "Diarization model pre-download skipped"

# Verify installations
RUN python3 -c "import torch; print('PyTorch version:', torch.__version__, 'CUDA available:', torch.cuda.is_available())"
RUN python3 -c "import faster_whisper; print('faster_whisper installed successfully')"
RUN python3 -c "import ivrit; print('ivrit package installed successfully')"
RUN python3 -c "from deepgram import DeepgramClient; print('Deepgram SDK installed')"
# Check if openai-whisper is installed (we don't want it)
RUN python3 -c "import importlib.util; spec = importlib.util.find_spec('whisper'); print('WARNING: openai-whisper found - this may conflict with faster-whisper') if spec else print('✓ openai-whisper not installed - using faster-whisper only (recommended)')"

# Copy application code
COPY app.py .
COPY static/ ./static/

# Create necessary directories for the application
# Note: We're NOT creating /root/.cache/whisper since we don't use openai-whisper
# Note: We're NOT creating whisper.cpp directories since we don't use GGML models
RUN mkdir -p /app/cache/audio && \
    mkdir -p /app/cache/downloads && \
    mkdir -p /app/cache/captures && \
    mkdir -p /app/logs

# Expose port
EXPOSE 8009

# Environment variables
ENV PYTHONUNBUFFERED=1
ENV PORT=8009
# Default to multilingual model (faster_whisper based - no whisper.cpp needed)
# For Hebrew-specific content, you can override with WHISPER_MODEL=ivrit-ct2
ENV WHISPER_MODEL=whisper-v3-turbo
ENV IVRIT_MODEL_NAME=ivrit-ai/whisper-large-v3-turbo-ct2
ENV IVRIT_MODEL_TYPE=ct2
ENV IVRIT_DEVICE=cuda
ENV IVRIT_COMPUTE_TYPE=float16
ENV IVRIT_BEAM_SIZE=5
# Audio processing
ENV AUDIO_CACHE_ENABLED=true
# CUDA settings
ENV CUDA_VISIBLE_DEVICES=0
ENV NVIDIA_VISIBLE_DEVICES=all
# Note: This app uses only faster_whisper with CT2 models for optimal performance
# GGML/whisper.cpp support has been completely removed

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=120s --retries=3 \
    CMD python3 -c "import requests; r = requests.get('http://localhost:8009/health', timeout=10); r.raise_for_status()" || exit 1

# Run the application
CMD ["python3", "app.py"]

##############################################
# Multi-stage variant with smaller final image
##############################################
FROM base AS runtime

# Clean up unnecessary files to reduce image size
RUN pip cache purge && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Set the entrypoint
ENTRYPOINT ["python3"]
CMD ["app.py"]